{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])  # true label\n",
    "x1 = torch.tensor([1.1]) # input feature\n",
    "w1 = torch.tensor([2.2]) # weight parameter\n",
    "b = torch.tensor([0.0])  # bias unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "print(x1)\n",
    "print(w1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x1 * w1 + b          # net input\n",
    "a = torch.sigmoid(z)     # activation & output\n",
    "\n",
    "print(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "# grad_L_w1 = grad(loss, w1)\n",
    "# grad_L_b = grad(loss, b)\n",
    "\n",
    "print(loss.requires_grad)  # Should be True if you manually set it, but not always needed\n",
    "print(loss.grad_fn)        # Should NOT be None if loss is part of a computation graph\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    "\n",
    "# print(f\"grad_L_w1:{grad_L_w1}\")\n",
    "# print(f\"grad_L_b:{grad_L_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "print(loss.requires_grad)  # Should be True if you manually set it, but not always needed\n",
    "print(loss.grad_fn)        # Should NOT be None if loss is part of a computation graph\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11011fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([10.0])\n",
    "\n",
    "# Forward computation\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6796ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "print(\"requires_grad:\", loss.requires_grad)  # ✅ Likely False\n",
    "print(\"grad_fn:\", loss.grad_fn)              # ✅ Should be not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_L_w1 = grad(loss, w1)\n",
    "# grad_L_b = grad(loss, b)\n",
    "\n",
    "print(loss.requires_grad)  # Should be True if you manually set it, but not always needed\n",
    "print(loss.grad_fn)        # Should NOT be None if loss is part of a computation graph\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edc19b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25545569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_L_w1: (tensor([-0.0898]),)\n",
      "grad_L_b: (tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "print(\"grad_L_w1:\", grad_L_w1)  # ✅ Likely False\n",
    "print(\"grad_L_b:\", grad_L_b)              # ✅ Should be not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce010f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu_0606",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
